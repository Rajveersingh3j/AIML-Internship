Day 1:
 1. What are the different types of missing data?

There are three main types of missing data:

 MCAR (Missing Completely at Random):

   The missingness is completely random and has no relationship with any feature or variable.
   Example: A sensor randomly fails to record temperature regardless of time or location.

 MAR (Missing at Random):

   The missingness is related to observed data, but not the missing data itself.
   Example: Income might be missing more often for younger people, but given the age, the missingness is random.

 MNAR (Missing Not at Random):

   The missingness is related to the value of the missing data itself.
   Example: People with high incomes might choose not to disclose their income.

---

 2. How do you handle categorical variables?

Common techniques:

 Label Encoding:

   Assigns each category a unique integer.
   Suitable for ordinal data (e.g., "Low" = 0, "Medium" = 1, "High" = 2).
   Risk: Can mislead algorithms into thinking there's a numerical relationship when it's nominal.

 One-Hot Encoding:

   Converts categories into binary columns.
   Suitable for nominal data (e.g., color → red, blue, green).
   Downside: Can lead to a high-dimensional dataset (curse of dimensionality).

 Frequency / Count Encoding:

   Replaces categories with the count or frequency of the category in the dataset.

 Target Encoding:

   Replaces each category with the mean of the target variable for that category.
   Risk of overfitting; usually needs cross-validation or regularization.

---

 3. What is the difference between normalization and standardization?

 Normalization (Min-Max Scaling):

   Scales values to a range of \[0, 1] or \[-1, 1].
   Formula: $(x - \min(x)) / (\max(x) - \min(x))$
   Sensitive to outliers.
   Used when data doesn't follow a Gaussian distribution.

 Standardization (Z-score Scaling):

   Centers the data around 0 with a standard deviation of 1.
   Formula: $(x - \mu) / \sigma$
   Less sensitive to outliers.
   Preferred when features follow a normal distribution.

---

 4. How do you detect outliers?

Several methods:

 Statistical Methods:

   Z-score: Values beyond ±3 standard deviations are considered outliers.
   IQR (Interquartile Range): Values below Q1 - 1.5×IQR or above Q3 + 1.5×IQR.

 Visualization:

   Boxplots, scatterplots, histograms.

 Model-Based Methods:

   Isolation Forest, DBSCAN, One-Class SVM.

 Domain Knowledge:

   Use business rules or thresholds specific to the application.

---

 5. Why is preprocessing important in ML?

 Improves Model Accuracy: Ensures data quality and consistency.
 Enables Convergence: Some algorithms require data in specific formats or distributions.
 Reduces Noise & Bias: Eliminates irrelevant or misleading information.
 Handles Missing/Imbalanced Data: Prevents biases or crashes during training.
 Speeds up Training: Removes unnecessary features and scales the data appropriately.

---

 6. What is one-hot encoding vs label encoding?

| Feature                          | One-Hot Encoding                    | Label Encoding               |
| -------------------------------- | ----------------------------------- | ---------------------------- |
| Converts                         | Category → multiple binary columns  | Category → numeric labels    |
| Suitable for                     | Nominal data                        | Ordinal data                 |
| Example ("Red", "Blue", "Green") | Red = \[1, 0, 0], Blue = \[0, 1, 0] | Red = 0, Blue = 1, Green = 2 |
| Risk                             | High dimensionality                 | Misinterpreted order         |

---

 7. How do you handle data imbalance?

Techniques include:

 Resampling:

   Oversampling (e.g., SMOTE): Duplicate or synthetically create minority class samples.
   Undersampling: Reduce majority class samples.

 Change Performance Metrics:

   Use metrics like F1-score, ROC-AUC, Precision-Recall instead of accuracy.

 Use Class Weights:

   Many ML algorithms support weighting classes (e.g., `class_weight='balanced'` in scikit-learn).

 Ensemble Techniques:

   Use specialized methods like Balanced Random Forest, EasyEnsemble.

---

 8. Can preprocessing affect model accuracy?

Absolutely. In fact, it's one of the most influential steps. Examples:

 Poor preprocessing:

   Unscaled features may bias distance-based algorithms (e.g., KNN, SVM).
   Missing values may crash models or skew results.
   Unencoded categorical data can’t be used in most algorithms.

 Good preprocessing:

   Makes learning easier and faster.
   Enhances generalization and accuracy.
   Reduces overfitting by cleaning noisy or irrelevant data.

---